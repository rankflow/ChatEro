PROMPT COMPLETO PARA CONFIGURAR OLLAMA CON MISTRAL
==================================================

Este prompt te ayudará a configurar Ollama con Mistral en tu servidor virtual Linux y conectarlo desde el backend de tu aplicación Chat IA + Avatares Eróticos.

PASO 1: INSTALAR OLLAMA EN TU SERVIDOR VIRTUAL LINUX
---------------------------------------------------

1. Conecta a tu servidor virtual Linux via SSH:
   ssh usuario@IP_DE_TU_PC

2. Descarga e instala Ollama:
   curl -fsSL https://ollama.ai/install.sh | sh

3. Verifica la instalación:
   ollama --version

PASO 2: DESCARGAR Y CONFIGURAR MISTRAL
--------------------------------------

1. Descarga el modelo Mistral (puedes elegir entre varias versiones):
   
   # Mistral 7B (más rápido, menos recursos):
   ollama pull mistral
   
   # O Mistral 7B Instruct (mejor para chat):
   ollama pull mistral:instruct
   
   # O Mistral 7B OpenOrca (muy bueno para contenido adulto):
   ollama pull mistral:openorca

2. Verifica que se descargó correctamente:
   ollama list

PASO 3: CONFIGURAR OLLAMA PARA ACCESO REMOTO
--------------------------------------------

1. Crea un archivo de configuración para Ollama:
   sudo mkdir -p /etc/ollama
   sudo nano /etc/ollama/ollama.conf

2. Añade esta configuración para permitir acceso remoto:
   OLLAMA_HOST=0.0.0.0:11434
   OLLAMA_ORIGINS=*

3. Reinicia el servicio de Ollama:
   sudo systemctl restart ollama

4. Verifica que está corriendo:
   sudo systemctl status ollama

PASO 4: CONFIGURAR FIREWALL Y PUERTOS
-------------------------------------

1. Abre el puerto 11434 (puerto por defecto de Ollama):
   sudo ufw allow 11434

2. O si usas iptables:
   sudo iptables -A INPUT -p tcp --dport 11434 -j ACCEPT

3. Verifica que el puerto está abierto:
   sudo netstat -tlnp | grep 11434

PASO 5: PROBAR LA CONEXIÓN LOCALMENTE
-------------------------------------

1. Prueba que Ollama responde localmente:
   curl -X POST http://localhost:11434/api/generate -d '{
     "model": "mistral",
     "prompt": "Hola, ¿cómo estás?",
     "stream": false
   }'

2. Si funciona, deberías ver una respuesta JSON con el texto generado.

PASO 6: CONFIGURAR COMO SERVICIO SYSTEMD (OPCIONAL)
---------------------------------------------------

1. Crea un archivo de servicio systemd:
   sudo nano /etc/systemd/system/ollama.service

2. Añade esta configuración:
   [Unit]
   Description=Ollama AI Service
   After=network.target

   [Service]
   Type=simple
   User=ollama
   Group=ollama
   ExecStart=/usr/local/bin/ollama serve
   Restart=always
   RestartSec=3
   Environment=OLLAMA_HOST=0.0.0.0:11434
   Environment=OLLAMA_ORIGINS=*

   [Install]
   WantedBy=multi-user.target

3. Habilita y inicia el servicio:
   sudo systemctl daemon-reload
   sudo systemctl enable ollama
   sudo systemctl start ollama

PASO 7: OBTENER LA IP DE TU PC
------------------------------

1. En tu servidor virtual Linux, ejecuta:
   ip addr show
   
   O
   
   hostname -I

2. Anota la IP de tu PC (será algo como 192.168.1.XXX)

PASO 8: PROBAR LA CONEXIÓN DESDE EL BACKEND
-------------------------------------------

1. En tu backend, verifica que el archivo .env tiene la configuración correcta:
   MISTRAL_API_URL=http://IP_DE_TU_PC:11434/api
   MISTRAL_MODEL=mistral

2. Prueba la conexión con curl desde tu Mac:
   curl -X POST http://IP_DE_TU_PC:11434/api/generate -d '{
     "model": "mistral",
     "prompt": "Hola, ¿cómo estás?",
     "stream": false
   }'

PASO 9: CONFIGURAR PROXY REVERSO (OPCIONAL, PARA MAYOR SEGURIDAD)
-----------------------------------------------------------------

Si quieres mayor seguridad, puedes configurar un proxy reverso con nginx:

1. Instala nginx:
   sudo apt update
   sudo apt install nginx

2. Crea configuración para Ollama:
   sudo nano /etc/nginx/sites-available/ollama

3. Añade esta configuración:
   server {
       listen 80;
       server_name tu-dominio.com;
       
       location / {
           proxy_pass http://localhost:11434;
           proxy_http_version 1.1;
           proxy_set_header Upgrade $http_upgrade;
           proxy_set_header Connection 'upgrade';
           proxy_set_header Host $host;
           proxy_set_header X-Real-IP $remote_addr;
           proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
           proxy_set_header X-Forwarded-Proto $scheme;
           proxy_cache_bypass $http_upgrade;
       }
   }

4. Habilita el sitio:
   sudo ln -s /etc/nginx/sites-available/ollama /etc/nginx/sites-enabled/
   sudo nginx -t
   sudo systemctl restart nginx

PASO 10: VERIFICAR TODO FUNCIONA
--------------------------------

1. Verifica que Ollama está corriendo:
   sudo systemctl status ollama

2. Verifica que responde en el puerto:
   curl http://localhost:11434/api/tags

3. Prueba una generación completa:
   curl -X POST http://localhost:11434/api/generate -d '{
     "model": "mistral",
     "prompt": "Escribe una historia erótica corta",
     "stream": false,
     "options": {
       "temperature": 0.8,
       "top_p": 0.9
     }
   }'

INFORMACIÓN QUE NECESITO DE TI
==============================

Una vez que hayas completado estos pasos, necesito que me proporciones:

1. La IP de tu PC (la que obtuviste en el paso 7)
2. Confirmación de que el servidor Ollama está corriendo
3. El resultado del comando: curl http://IP_DE_TU_PC:11434/api/tags
4. Cualquier error que hayas encontrado durante el proceso

CONFIGURACIÓN FINAL EN EL BACKEND
=================================

Una vez que tengas todo funcionando, actualizaré la configuración en tu backend para usar tu servidor Mistral en lugar de OpenAI.

El archivo .env ya está configurado con las variables necesarias:
- MISTRAL_API_URL=http://IP_DE_TU_PC:11434/api
- MISTRAL_MODEL=mistral

Y el servicio MistralService ya está implementado para conectarse a tu servidor.

¿Necesitas ayuda con algún paso específico o tienes alguna pregunta sobre la configuración? 